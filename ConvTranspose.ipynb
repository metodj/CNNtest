{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "\n",
    "from lasagne import init\n",
    "from lasagne import nonlinearities\n",
    "from lasagne.utils import as_tuple\n",
    "from lasagne.theano_extensions import conv\n",
    "\n",
    "from lasagne.layers.base import Layer\n",
    "\n",
    "def conv_output_length(input_length, filter_size, stride, pad=0):\n",
    "    if input_length is None:\n",
    "        return None\n",
    "    if pad == 'valid':\n",
    "        output_length = input_length - filter_size + 1\n",
    "    elif pad == 'full':\n",
    "        output_length = input_length + filter_size - 1\n",
    "    elif pad == 'same':\n",
    "        output_length = input_length\n",
    "    elif isinstance(pad, int):\n",
    "        output_length = input_length + 2 * pad - filter_size + 1\n",
    "    else:\n",
    "        raise ValueError('Invalid pad: {0}'.format(pad))\n",
    "\n",
    "    # This is the integer arithmetic equivalent to\n",
    "    # np.ceil(output_length / stride)\n",
    "    output_length = (output_length + stride - 1) // stride\n",
    "\n",
    "    return output_length\n",
    "\n",
    "def conv_input_length(output_length, filter_size, stride, pad=0):\n",
    "      \n",
    "    if output_length is None:\n",
    "        return None\n",
    "    if pad == 'valid':\n",
    "        pad = 0\n",
    "    elif pad == 'full':\n",
    "        pad = filter_size - 1\n",
    "    elif pad == 'same':\n",
    "        pad = filter_size // 2\n",
    "    if not isinstance(pad, int):\n",
    "        raise ValueError('Invalid pad: {0}'.format(pad))\n",
    "    return (output_length - 1) * stride - 2 * pad + filter_size\n",
    "\n",
    "\n",
    "class BaseConvLayer(Layer):\n",
    "    def __init__(self, incoming, num_filters, filter_size, stride=1, pad=0,\n",
    "                 untie_biases=False,\n",
    "                 W=init.GlorotUniform(), b=init.Constant(0.),\n",
    "                 nonlinearity=nonlinearities.rectify, flip_filters=True,\n",
    "                 n=None, **kwargs):\n",
    "        super(BaseConvLayer, self).__init__(incoming, **kwargs)\n",
    "        if nonlinearity is None:\n",
    "            self.nonlinearity = nonlinearities.identity\n",
    "        else:\n",
    "            self.nonlinearity = nonlinearity\n",
    "\n",
    "        if n is None:\n",
    "            n = len(self.input_shape) - 2\n",
    "        elif n != len(self.input_shape) - 2:\n",
    "            raise ValueError(\"Tried to create a %dD convolution layer with \"\n",
    "                             \"input shape %r. Expected %d input dimensions \"\n",
    "                             \"(batchsize, channels, %d spatial dimensions).\" %\n",
    "                             (n, self.input_shape, n+2, n))\n",
    "        self.n = n\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = as_tuple(filter_size, n, int)\n",
    "        self.flip_filters = flip_filters\n",
    "        self.stride = as_tuple(stride, n, int)\n",
    "        self.untie_biases = untie_biases\n",
    "\n",
    "        if pad == 'same':\n",
    "            if any(s % 2 == 0 for s in self.filter_size):\n",
    "                raise NotImplementedError(\n",
    "                    '`same` padding requires odd filter size.')\n",
    "        if pad == 'valid':\n",
    "            self.pad = as_tuple(0, n)\n",
    "        elif pad in ('full', 'same'):\n",
    "            self.pad = pad\n",
    "        else:\n",
    "            self.pad = as_tuple(pad, n, int)\n",
    "\n",
    "        self.W = self.add_param(W, self.get_W_shape(), name=\"W\")\n",
    "        if b is None:\n",
    "            self.b = None\n",
    "        else:\n",
    "            if self.untie_biases:\n",
    "                biases_shape = (num_filters,) + self.output_shape[2:]\n",
    "            else:\n",
    "                biases_shape = (num_filters,)\n",
    "            self.b = self.add_param(b, biases_shape, name=\"b\",\n",
    "                                    regularizable=False)\n",
    "\n",
    "    def get_W_shape(self):\n",
    "        \"\"\"Get the shape of the weight matrix `W`.\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of int\n",
    "            The shape of the weight matrix.\n",
    "        \"\"\"\n",
    "        num_input_channels = self.input_shape[1]\n",
    "        return (self.num_filters, num_input_channels) + self.filter_size\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        pad = self.pad if isinstance(self.pad, tuple) else (self.pad,) * self.n\n",
    "        batchsize = input_shape[0]\n",
    "        return ((batchsize, self.num_filters) +\n",
    "                tuple(conv_output_length(input, filter, stride, p)\n",
    "                      for input, filter, stride, p\n",
    "                      in zip(input_shape[2:], self.filter_size,\n",
    "                             self.stride, pad)))\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        conved = self.convolve(input, **kwargs)\n",
    "\n",
    "        if self.b is None:\n",
    "            activation = conved\n",
    "        elif self.untie_biases:\n",
    "            activation = conved + T.shape_padleft(self.b, 1)\n",
    "        else:\n",
    "            activation = conved + self.b.dimshuffle(('x', 0) + ('x',) * self.n)\n",
    "\n",
    "        return self.nonlinearity(activation)\n",
    "\n",
    "    def convolve(self, input, **kwargs):\n",
    "        \"\"\"\n",
    "        Symbolically convolves `input` with ``self.W``, producing an output of\n",
    "        shape ``self.output_shape``. To be implemented by subclasses.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : Theano tensor\n",
    "            The input minibatch to convolve\n",
    "        **kwargs\n",
    "            Any additional keyword arguments from :meth:`get_output_for`\n",
    "        Returns\n",
    "        -------\n",
    "        Theano tensor\n",
    "            `input` convolved according to the configuration of this layer,\n",
    "            without any bias or nonlinearity applied.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"BaseConvLayer does not implement the \"\n",
    "                                  \"convolve() method. You will want to \"\n",
    "                                  \"use a subclass such as Conv2DLayer.\")\n",
    "\n",
    "class Conv3DLayerTransposed(BaseConvLayer):\n",
    "\n",
    "    #pad spremenim v crop, flip_filters nastavim na True, spremenim super()\n",
    "    def __init__(self, incoming, num_filters, filter_size, stride=(1, 1, 1),\n",
    "                 crop=0, untie_biases=False,\n",
    "                 W=init.GlorotUniform(), b=init.Constant(0.),\n",
    "                 nonlinearity=nonlinearities.rectify, flip_filters=False,\n",
    "                 convolution=T.nnet.ConvTransp3D, output_size=None, **kwargs):\n",
    "        super(Conv3DLayerTransposed, self).__init__(incoming, num_filters, filter_size,\n",
    "                                          stride, crop, untie_biases, W, b,\n",
    "                                          nonlinearity, flip_filters, n=3,\n",
    "                                          **kwargs)\n",
    "        self.crop = self.pad\n",
    "        del self.pad\n",
    "        self.convolution = convolution\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def get_W_shape(self):\n",
    "        num_input_channels = self.input_shape[1]\n",
    "        # first two sizes are swapped compared to a forward convolution\n",
    "        return (num_input_channels, self.num_filters) + self.filter_size\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        #if self.output_size is not None:\n",
    "         #   size = self.output_size\n",
    "        #     if isinstance(self.output_size, T.Variable):\n",
    "        #        size = (None, None)\n",
    "        #    return input_shape[0], self.num_filters, size[0], size[1]\n",
    "\n",
    "        # If self.output_size is not specified, return the smallest shape\n",
    "        # when called from the constructor, self.crop is still called self.pad:\n",
    "        crop = getattr(self, 'crop', getattr(self, 'pad', None))\n",
    "        crop = crop if isinstance(crop, tuple) else (crop,) * self.n\n",
    "        batchsize = input_shape[0]\n",
    "        return ((batchsize, self.num_filters) +\n",
    "                tuple(conv_input_length(input, filter, stride, p)\n",
    "                      for input, filter, stride, p\n",
    "                      in zip(input_shape[2:], self.filter_size,\n",
    "                             self.stride, crop)))\n",
    "\n",
    "    #pad v crop, filter_flip nastavim na not \n",
    "    #def convolve(self, input, **kwargs):\n",
    "    #    border_mode = 'half' if self.crop == 'same' else self.crop\n",
    "    #    conved = self.convolution(input, self.W, self.input_shape, self.get_W_shape(),\n",
    "    #                              subsample=self.stride,\n",
    "    #                              border_mode=border_mode,\n",
    "    #                              filter_flip=not self.flip_filters)\n",
    "    #    return conved\n",
    "    \n",
    "    def convolve(self, input, **kwargs):\n",
    "        border_mode = 'half' if self.crop == 'same' else self.crop\n",
    "        op = T.nnet.abstract_conv.AbstractConv3d_gradInputs(\n",
    "            imshp=self.output_shape,\n",
    "            kshp=self.get_W_shape(),\n",
    "            subsample=self.stride, border_mode=border_mode,\n",
    "            filter_flip=not self.flip_filters)\n",
    "        output_size = self.output_shape[2:]\n",
    "        if isinstance(self.output_size, T.Variable):\n",
    "            output_size = self.output_size\n",
    "        elif any(s is None for s in output_size):\n",
    "            output_size = self.get_output_shape_for(input.shape)[2:]\n",
    "        conved = op(self.W, input, output_size)\n",
    "        return conved"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
